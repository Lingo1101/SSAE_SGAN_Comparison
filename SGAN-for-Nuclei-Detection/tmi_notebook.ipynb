{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py [OPTIONS] COMMAND [ARGS]...\n",
      "Try \"ipykernel_launcher.py --help\" for help.\n",
      "\n",
      "Error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iancarvalho/Documents/sgan_research/SSAE_SGAN_Comparison/SGAN-for-Nuclei-Detection/.env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# %load model-TMI.py\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, average_precision_score\n",
    "from skimage.transform import resize\n",
    "import scipy.io\n",
    "\n",
    "import click\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import mmappickle\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "seed = 19680801\n",
    "np.random.seed(seed)\n",
    "\n",
    "class SGAN():\n",
    "    def __init__(self):\n",
    "        # TMI input shape (after resized)is 32x32x3\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.num_classes = 2\n",
    "        self.training_history = {\n",
    "                'D_loss': [],\n",
    "                'D_acc': [],\n",
    "                'G_loss': [],\n",
    "                'G_acc': [],\n",
    "                }\n",
    "\n",
    "        # While previous GAN work has used momentum to accelerate training, we used the Adam optimizer\n",
    "        # (Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001,\n",
    "        # to be too high, using 0.0002 instead. Additionally, we found leaving the momentum term Î²1 at the\n",
    "        # suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped\n",
    "        # stabilize training\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build discriminator's model\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        # Compile discriminator's model, i.e. define its learning process\n",
    "        # binary crossentropy is used to distinguish among real or fake samples\n",
    "        # categorical entropy is to distinguish among which real category is (nuclei or non-nuclei)\n",
    "        self.discriminator.compile(\n",
    "                loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
    "                loss_weights=[0.5, 0.5],\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid, _ = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(\n",
    "                loss=['binary_crossentropy'], \n",
    "                optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        # This model replaced any pooling layers with strided convolutions\n",
    "        # Allowing it to learn its own spatial upsampling\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 8 * 8, activation='relu', input_dim=100))\n",
    "        \n",
    "        model.add(Reshape((8, 8, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        # fractionally-strided convolution, do not confuse with deconvolution operation\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding='same'))\n",
    "        # using a bounded activation allowed the model to learn more quickly to saturate\n",
    "        # and cover the color space of the training distribution\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        #upsampling is the opposite to pooling. Repeats the rows and columns of the data\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        #flatten to the amount of channels\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        \n",
    "#        plot_path = 'generator.png'\n",
    "#        plot_model(model, to_file=plot_path, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        noise = Input(shape=(100,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        # This model replaced any pooling layers with strided convolutions\n",
    "        # Allowing it to learn its own spatial downsampling\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # A Sequential model is a linear stack of layers.\n",
    "        model = Sequential()\n",
    "\n",
    "        # Create a Sequential model by simply adding layers via the .add() method\n",
    "        # 32 filters, 3x3 kernel size, stride 2, input_shape is 28x28x1, same: pad so the output and input size are equal\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
    "        # f(x) = alpha * x for x < 0, f(x) = x for x >= 0.\n",
    "        # Leaky rectified activation worked well, especially for higher resolution modeling.\n",
    "        # This is in contrast to the original GAN paper, which used the maxout activation\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # drops 25% of the input units\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
    "        #A zero-padding layer. Adds rows and columns of zeros to the image\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        # Normalize the activations of the previous layer at each batch to reduce its covariance shift,\n",
    "        # i.e., the amount that the distribution of each layer shift around.\n",
    "\n",
    "        # This helps deal with training problems that arise due to poor initialization and helps gradient flow in deeper models.\n",
    "        # This proved critical to get deep generators to begin learning, preventing the generator from collapsing all samples\n",
    "        # to a single point which is a common failure mode observed in GANs.\n",
    "        #\n",
    "        # Directly applying batchnorm to all layers, however, resulted in sample oscillation and model instability.\n",
    "        # This was avoided by not applying batchnorm to the generator output layer and the discriminator input layer\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        #model.summary()\n",
    "#        plot_path = 'discriminator.png'\n",
    "#        plot_model(model, to_file=plot_path, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "        # instantiate a Keras tensor\n",
    "        img = Input(shape=img_shape)\n",
    "        features = model(img)\n",
    "\n",
    "        # valid indicates if the image is real or fake\n",
    "        valid = Dense(1, activation='sigmoid')(features)\n",
    "        # iff the image is real, label indicates which type of image it is\n",
    "        label = Dense(self.num_classes+1, activation='softmax')(features)\n",
    "\n",
    "        # Given an img (x)  and a label(y), instantiate a Model.\n",
    "        # Once instantiated, this model will include all layers required in the computation of y given x.\n",
    "        return Model(img, [valid, label])\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, epochs, batch_size, save_interval):\n",
    "\n",
    "        # delete directory if exist and create it\n",
    "        shutil.rmtree('TMI_generators_output', ignore_errors=True)\n",
    "        os.makedirs('TMI_generators_output')\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        # Class weights:\n",
    "        # To balance the difference in occurences of class labels.\n",
    "        # 50% of labels that D trains on are 'fake'.\n",
    "        # Weight = 1 / frequency\n",
    "        cw1 = {0: 1, 1: 1}\n",
    "        cw2 = {i: self.num_classes / half_batch for i in range(self.num_classes)}\n",
    "        cw2[self.num_classes] = 1 / half_batch\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Training the Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Draw random samples from a Gaussian distribution.\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            valid = np.ones((half_batch, 1))\n",
    "            fake = np.zeros((half_batch, 1))\n",
    "\n",
    "            # Convert labels to categorical one-hot encoding\n",
    "            labels = to_categorical(y_train[idx], num_classes=self.num_classes+1)\n",
    "            fake_labels = to_categorical(np.full((half_batch, 1), self.num_classes), num_classes=self.num_classes+1)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and fakes as zeros)\n",
    "            # train_on_batch: Single gradient update over one batch of samples\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, labels], class_weight=[cw1, cw2])\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, fake_labels], class_weight=[cw1, cw2])\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Training the Generator\n",
    "            # ---------------------\n",
    "            validity = np.ones((batch_size, 1))\n",
    "            \n",
    "            for i in range(10):\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "                # Train the generator (wants discriminator to mistake images as real)\n",
    "                g_loss = self.combined.train_on_batch(noise, validity, class_weight=[cw1, cw2])\n",
    "\n",
    "            self.training_history['D_loss'].append(d_loss[0]);\n",
    "            self.training_history['D_acc'].append(100*d_loss[3]);\n",
    "            self.training_history['G_loss'].append(g_loss);\n",
    "            self.training_history['G_acc'].append(100*d_loss[4]);\n",
    "\n",
    "            print ('%d: Training D [loss: %.4f, acc: %.2f%% ] - G [loss: %.4f, acc: %.2f%%]' % (epoch, d_loss[0], 100*d_loss[3], g_loss, 100*d_loss[4]))\n",
    "            self.evaluate_discriminator(X_test, y_test)\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "                \n",
    "    def evaluate_discriminator(self, X_test, y_test):\n",
    "        valid = np.ones((y_test.shape[0], 1))\n",
    "\n",
    "        # Convert labels to categorical one-hot encoding\n",
    "        labels = to_categorical(y_test, num_classes=self.num_classes+1)\n",
    "\n",
    "        #  Evaluating the trained Discriminator\n",
    "        scores = self.discriminator.evaluate(X_test, [valid, labels], verbose=0)\n",
    "\n",
    "        print('Evaluating D [loss:  %.4f, bi-loss: %.4f, cat-loss: %.4f, bi-acc: %.2f%%, cat-acc: %.2f%%]\\n' %\n",
    "              (scores[0], scores[1], scores[2], scores[3]*100, scores[4]*100))\n",
    "#        print('\\nEvaluating D [loss:  %.4f, acc: %.2f%%]' % (scores[0], scores[3]*100))\n",
    "\n",
    "        return (scores[0], scores[3]*100)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images from [-1..1] to [0..1] just to display purposes.\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig('./TMI_generators_output/tmi_%d.png' % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = './TMI_saved_models/%s.json' % model_name\n",
    "            weights_path = './TMI_saved_models/%s_weights.hdf5' % model_name\n",
    "            options = {'file_arch': model_path,\n",
    "                        'file_weight': weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        shutil.rmtree('TMI_saved_models', ignore_errors=True)\n",
    "        os.makedirs('TMI_saved_models')\n",
    "\n",
    "        save(self.generator, 'TMI_gan_generator')\n",
    "        save(self.discriminator, 'TMI_gan_discriminator')\n",
    "        save(self.combined, 'TMI_gan_adversarial')\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "        plt.title('Training History')\n",
    "        # summarize history for G and D accuracy\n",
    "        axs[0].plot(range(1,len(self.training_history['D_acc'])+1),self.training_history['D_acc'])\n",
    "        axs[0].plot(range(1,len(self.training_history['G_acc'])+1),self.training_history['G_acc'])\n",
    "        axs[0].set_title('D and G Accuracy')\n",
    "        axs[0].set_ylabel('Accuracy')\n",
    "        axs[0].set_xlabel('Epoch')\n",
    "        axs[0].set_xticks(np.arange(1,len(self.training_history['D_acc'])+1),len(self.training_history['D_acc'])/10)\n",
    "        axs[0].set_yticks([n for n in range(0, 101,10)])\n",
    "        axs[0].legend(['Discriminator', 'Generator'], loc='best')\n",
    "\n",
    "        # summarize history for G and D loss\n",
    "        axs[1].plot(range(1,len(self.training_history['D_loss'])+1),self.training_history['D_loss'])\n",
    "        axs[1].plot(range(1,len(self.training_history['G_loss'])+1),self.training_history['G_loss'])\n",
    "        axs[1].set_title('D and G Loss')\n",
    "        axs[1].set_ylabel('Loss')\n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_xticks(np.arange(1,len(self.training_history['G_loss'])+1),len(self.training_history['G_loss'])/10)\n",
    "        axs[1].legend(['Discriminator', 'Generator'], loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X_test, y_test):\n",
    "\n",
    "        # Generating a predictions from the discriminator over the testing dataset\n",
    "        y_pred = self.discriminator.predict(X_test)\n",
    "        print(y_pred[1][:,:-1])\n",
    "        # Formating predictions to remove the one_hot_encoding format\n",
    "        y_pred = np.argmax(y_pred[1][:,:-1], axis=1)\n",
    "\n",
    "        print ('\\nOverall accuracy: %f%% \\n' % (accuracy_score(y_test, y_pred) * 100))\n",
    "        print ('\\nAveP: %f%% \\n' % (average_precision_score(y_test, y_pred) * 100))\n",
    "        \n",
    "        # Calculating and ploting a Classification Report\n",
    "        class_names = ['Non-nunclei', 'Nuclei']\n",
    "        print('Classification report:\\n %s\\n'\n",
    "              % (classification_report(y_test, y_pred, target_names=class_names)))\n",
    "\n",
    "        # Calculating and ploting Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "#        print('Confusion matrix:\\n%s' % cm)\n",
    "\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm, class_names, title='Confusion matrix, without normalization')\n",
    "\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm, class_names, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.discriminator.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    def predict_nms(self, images):\n",
    "        pass\n",
    "\n",
    "    def load_weights(self):\n",
    "        # load weights into new model\n",
    "        self.generator.load_weights('./TMI_saved_models/TMI_gan_generator_weights.hdf5')\n",
    "        self.discriminator.load_weights('./TMI_saved_models/TMI_gan_discriminator_weights.hdf5')\n",
    "        # self.combined.load_weights('./TMI_saved_models/TMI_gan_adversarial_weights.hdf5')\n",
    "        print('Weights loaded from disk')\n",
    "\n",
    "def is_nuclei(cell):\n",
    "    if len(np.transpose(np.nonzero(cell))) == 0:\n",
    "        return False\n",
    "    p_n = np.array(np.transpose(np.nonzero(cell))[0][:2])\n",
    "    p_c = np.array([17, 17])\n",
    "    dist = np.linalg.norm(p_n - p_c)\n",
    "# #   if dist < 15 nuclei_arr.append(dist, p_n, or p_c?)\n",
    "    return dist < 17\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    '''\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def load_TMI_data():\n",
    "    # Load the dataset\n",
    "    dataset = scipy.io.loadmat('TMI2015/training/training.mat')\n",
    "\n",
    "    # cv2.imshow('train_x', dataset['train_x'][0])\n",
    "    cv2.waitKey(0)\n",
    "    # Split into train and test. Values are in range [0..1] as float64\n",
    "    X_train = np.transpose(dataset['train_x'], (3, 0, 1, 2))\n",
    "    print(X_train.shape)\n",
    "    y_train = list(dataset['train_y'][0])\n",
    "    \n",
    "    X_test = np.transpose(dataset['test_x'], (3, 0, 1, 2))\n",
    "    y_test = list(dataset['test_y'][0])\n",
    "    \n",
    "    # Change shape and range. \n",
    "    y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "    y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "\n",
    "#   1-> 0 : Non-nucleus. 2 -> 1: Nucleus\n",
    "    y_test -= 1\n",
    "    y_train -= 1\n",
    "\n",
    "    # Resize to 32x32\n",
    "    X_train_resized = np.empty([X_train.shape[0], 32, 32, X_train.shape[3]])\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train_resized[i] = resize(X_train[i], (32, 32, 3), mode='reflect')\n",
    "\n",
    "    X_test_resized = np.empty([X_test.shape[0], 32, 32, X_test.shape[3]])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test_resized[i] = resize(X_test[i], (32, 32, 3), mode='reflect')\n",
    "    \n",
    "    # Normalize images from [0..1] to [-1..1]\n",
    "    X_train_resized = 2 * X_train_resized - 1\n",
    "    X_test_resized = 2 * X_test_resized - 1\n",
    "    return X_train_resized, y_train, X_test_resized, y_test\n",
    "\n",
    "def train_model():\n",
    "    X_train, y_train, X_test, y_test = load_TMI_data()\n",
    "\n",
    "#    Instanciate a compiled model\n",
    "    sgan = SGAN()\n",
    "\n",
    "#    sgan.load_weights()\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    epochs=200\n",
    "    # Fit/Train the model\n",
    "    sgan.train(X_train, y_train, X_test, y_test, epochs, batch_size=32, save_interval=5)\n",
    "\n",
    "    end = time.time()\n",
    "    print ('\\nTraining time: %0.1f minutes \\n' % ((end-start) / 60))\n",
    "\n",
    "    #saved the trained model\n",
    "    sgan.save_model()\n",
    "\n",
    "#    plot training graph\n",
    "    sgan.plot_training_history()\n",
    "\n",
    "\n",
    "def image_for_prefix(prf):\n",
    "    base_path = prf + '.tif'\n",
    "    block_path = prf + '_block.tif'\n",
    "    cell_path = prf + '_cell.tif'\n",
    "    base_img = cv2.imread(base_path)\n",
    "    block_img = cv2.imread(block_path, cv2.IMREAD_GRAYSCALE)\n",
    "    cell_img = cv2.imread(cell_path)\n",
    "    # x, y \n",
    "\n",
    "    block = cv2.findNonZero(block_img).squeeze()\n",
    "\n",
    "    x,y = block[0] + 3\n",
    "    x_0,y_0 = block[-1] - 2\n",
    "\n",
    "    crop = base_img[y:y_0, x:x_0]\n",
    "    cell = cell_img[y:y_0, x:x_0]\n",
    "    return crop, cell\n",
    "\n",
    "def sliding_windows(img_size, window_size, step):\n",
    "    max_x, max_y = img_size\n",
    "    w, h = window_size\n",
    "    x,y = 0,0\n",
    "    windows = []\n",
    "    for x_0 in range(x, max_x - w, step):\n",
    "        for y_0 in range(y, max_y - h, step):\n",
    "            windows.append((y_0,x_0,y_0+h,x_0+w))\n",
    "\n",
    "    return np.array(windows)\n",
    "\n",
    "def nms(windows, proba, thresh, overlap_rate):\n",
    "    pick = []\n",
    "    boxes = windows\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    # proba = proba[np.where(proba < thresh)]\n",
    "    idxs = np.argsort(proba)\n",
    "    t = proba[idxs]\n",
    "    idxs = np.delete(idxs, np.where(t < thresh)[0])\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    " \n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    " \n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    " \n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    " \n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlap_rate)[0])))\n",
    " \n",
    "    return pick\n",
    "\n",
    "\n",
    "def prepare_patches(patches):\n",
    "    X_test = np.asarray(patches)\n",
    "    X_test_resized = np.empty([X_test.shape[0], 32, 32, X_test.shape[3]])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test_resized[i] = resize(X_test[i], (32, 32, 3), mode='reflect')\n",
    "    \n",
    "    # Normalize images from [0..1] to [-1..1]\n",
    "    X_test_resized = 2 * X_test_resized - 1\n",
    "    return X_test_resized\n",
    "\n",
    "@click.group()\n",
    "def cli():\n",
    "   pass\n",
    "\n",
    "@cli.command()\n",
    "def test_predict():\n",
    "    # X_test, y_test = load_TMI_test_data()\n",
    "    X_train, y_train, X_test, y_test = load_TMI_data()\n",
    "\n",
    "    print (\"Loaded test data\")\n",
    "    sgan = SGAN()\n",
    "\n",
    "    sgan.load_weights()\n",
    "\n",
    "    sgan.evaluate_discriminator(X_test, y_test)\n",
    "\n",
    "    sgan.predict(X_test, y_test)\n",
    "\n",
    "@cli.command()\n",
    "@click.option('-p', '--path', \n",
    "    type=click.Path(exists=True),\n",
    "    help='Tests the current model against a provided dataset')\n",
    "def test_model(path):\n",
    "    print(path)\n",
    "    sgan = SGAN()\n",
    "    sgan.load_weights()\n",
    "\n",
    "    m = mmappickle.mmapdict(path, readonly=True)\n",
    "    all_preds = None\n",
    "    all_tests = None\n",
    "    print(m.keys())\n",
    "    for key in list(m.keys())[:10]:\n",
    "        print(key)\n",
    "        d = m[key]\n",
    "        crop = d['crop']\n",
    "        cell = d['cell']\n",
    "        print(d.keys())\n",
    "\n",
    "        windows = sliding_windows((400, 400), (34, 34), 6)[:1]\n",
    "        patches = [crop[w[0]:w[2], w[1]:w[3]] for w in windows]\n",
    "        cell_patches = [cell[w[0]:w[2], w[1]:w[3]] for w in windows]\n",
    "        y_test = np.array([is_nuclei(n) for n in cell_patches])\n",
    "        try:\n",
    "            y_proba = sgan.predict_proba(prepare_patches(patches))\n",
    "            print(y_proba)\n",
    "            y_proba = y_proba[1][:,:-1]\n",
    "            print(y_proba)\n",
    "            y_pred = np.argmax(y_proba, axis=1)\n",
    "            print(y_pred)\n",
    "        except Exception as e:\n",
    "            print(\"Erro\")\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        return\n",
    "    #     print(np.argwhere(y_pred == 1))\n",
    "    #     return\n",
    "        nuclei_picks = nms(windows, y_proba[:,1], 0.1, 0.3)\n",
    "        print(nuclei_picks)\n",
    "    #     print(\"Nuclei picks\")\n",
    "        non_nuclei_picks  = nms(windows, y_proba[:,0], 0.1, 0.3)\n",
    "    #     print(nuclei_picks)\n",
    "    #     print(\"Non nuclei picks\")\n",
    "    #     print(non_nuclei_picks)\n",
    "        picks =  np.concatenate((nuclei_picks, non_nuclei_picks))\n",
    "        print(picks)\n",
    "    #     print(picks)\n",
    "        y_test = y_test[picks]\n",
    "        y_pred = y_pred[picks]\n",
    "\n",
    "        if all_preds is None:\n",
    "            all_preds = y_pred\n",
    "            all_tests = y_test\n",
    "        else:\n",
    "            all_preds = np.concatenate((all_preds, y_pred))\n",
    "            all_tests = np.concatenate((all_tests, y_test))\n",
    "\n",
    "    print ('\\nOverall accuracy: %f%% \\n' % (accuracy_score(all_preds, all_tests) * 100))\n",
    "    print ('\\nAveP: %f%% \\n' % (average_precision_score(all_preds, all_tests) * 100))\n",
    "    \n",
    "    # Calculating and ploting a Classification Report\n",
    "    class_names = ['Non-nunclei', 'Nuclei']\n",
    "    print('Classification report:\\n %s\\n'\n",
    "          % (classification_report(all_preds, all_tests, target_names=class_names)))\n",
    "\n",
    "    # Calculating and ploting Confusion Matrix\n",
    "    # cm = confusion_matrix(all_preds, all_tests)\n",
    "#        print('Confusion matrix:\\n%s' % cm)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plot_confusion_matrix(cm, class_names, title='Confusion matrix, without normalization')\n",
    "\n",
    "    # plt.figure()\n",
    "    # plot_confusion_matrix(cm, class_names, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "\n",
    "    # sgan.evaluate_discriminator(X_test, y_test)\n",
    "    # sgan.predict(X_test, y_test)\n",
    "\n",
    "@cli.command()\n",
    "@click.option('-d', type=click.Path(exists=True),\n",
    "    help='Directory to read images from')\n",
    "@click.option('-n', type=int, default=500,\n",
    "    help='Number of images in the sample')\n",
    "@click.option('-o', 'output', default='output',\n",
    "    help='Output Folder')\n",
    "def create_dataset(d, n, output):\n",
    "    prefixes = [d + '/' + p.split('.')[0] for p in os.listdir(d) if '_' not in p and 'tif' in p]\n",
    "    prefixes = np.random.choice(prefixes, n, replace=False)\n",
    "    out_path = output + '/out'\n",
    "    if os.path.isfile(out_path):\n",
    "        os.remove(out_path)\n",
    "\n",
    "    m = mmappickle.mmapdict(out_path)\n",
    "    for prefix in prefixes:\n",
    "        crop, cells = image_for_prefix(prefix)\n",
    "        d = {\n",
    "            'crop': crop,\n",
    "            'cell': cells\n",
    "        }\n",
    "\n",
    "        key = os.path.basename(prefix)\n",
    "        m[key] = d\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
